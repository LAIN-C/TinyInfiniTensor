# TinyInfiniTensor 作业技术核心总结

本文总结 TinyInfiniTensor 训练营作业（作业一到作业八）涉及的关键技术点，便于复盘与扩展。

## 1. 内存分配器（Allocator）

### 1.1 问题建模：图执行的“静态内存规划”

TinyInfiniTensor 的 `Allocator` 并不立刻向系统申请内存，而是先**模拟分配**，得到每个 tensor 对应的“偏移量（offset）”。

- `alloc(size)`：返回一个偏移量，表示该块在整段大内存中的起始位置。
- `free(offset, size)`：回收一段偏移区间。
- `getPtr()`：在规划完后一次性 `runtime->alloc(peak)`，拿到真实内存基址。

这种做法对应编译器/运行时里的经典思路：
- 先做 memory planning，求峰值 `peak`
- 再一次性分配，减少碎片与系统调用

### 1.2 Free list + 合并（Coalescing）

为了复用回收的内存，需要维护“空闲块（free block）”。常用数据结构是 free list。

实现要点：
- 对齐（alignment）：所有 size/offset 都按 `alignment` 向上取整，避免类型对齐问题。
- 分配策略：从 free list 找到能容纳的块（例如 first-fit），必要时**拆分（split）**。
- 释放策略：插入 free block 后与相邻空闲块**合并（coalesce）**，减少碎片。
- 末尾回收：如果空闲块触达当前 heap 末尾，可把 heapEnd 往回缩（shrink），这直接影响后续大块分配是否能复用同一 offset。

对应代码位置：
- 分配器声明与 free block 数据结构：[include/core/allocator.h](../include/core/allocator.h)
- 分配/回收算法实现：[src/core/allocator.cc](../src/core/allocator.cc)

## 2. 形状推导（Shape Inference）

形状推导的目标：根据输入 tensor 的 shape 与算子属性，计算输出 shape。

### 2.1 Transpose

- permutation 语义：`output[i] = input[perm[i]]`
- rank 与 perm 必须一致。

对应代码： [src/operators/transpose.cc](../src/operators/transpose.cc)

### 2.2 Clip

- Clip 是逐元素（element-wise）算子，不改变 shape。

对应代码： [src/operators/unary.cc](../src/operators/unary.cc)

### 2.3 Cast（形状 + 数据类型）

- Cast 不改变 shape。
- Cast 改变 dtype：输出 dtype 由 castType 决定。

对应代码： [src/operators/unary.cc](../src/operators/unary.cc)

### 2.4 Concat

约束：
- 所有输入 rank 必须相同
- 除 concat 轴之外，所有维度必须一致
- 输出该轴维度为输入该轴维度之和

对应代码： [src/operators/concat.cc](../src/operators/concat.cc)

### 2.5 Matmul（含 batch broadcast + transpose flag）

Matmul 的形状推导可分解为：
1) 取 batch 维（除最后两维外的所有前导维度），对 A、B 的 batch 做广播得到 `outBatch`
2) 根据 `transA/transB` 选择矩阵维度：
   - A 的有效矩阵为 $(M, K)$
   - B 的有效矩阵为 $(K, N)$
3) 输出 shape 为 `outBatch + [M, N]`

广播规则参考 ONNX broadcasting（与 numpy 类似）。

对应代码：
- [src/operators/matmul.cc](../src/operators/matmul.cc)
- [src/utils/operator_utils.cc](../src/utils/operator_utils.cc)

## 3. 双向广播（Bidirectional Broadcasting）

广播用于 element-wise 算子与 batch matmul。

规则（从尾部维度对齐）：
- 维度相等：结果为该值
- 某一方为 1：结果为另一方
- 否则不合法

对应代码： [src/utils/operator_utils.cc](../src/utils/operator_utils.cc)

参考：ONNX Broadcasting 文档（训练营作业介绍里给出链接）。

## 4. 计算图内存分配（Graph::dataMalloc）

核心思想：基于拓扑序做简单的**张量生命周期分析（liveness）**。

流程：
1) 拓扑排序，保证生产者在消费者之前
2) 统计每个 tensor 的 remaining uses（被多少算子消费）
3) 分配：
   - 图输入（source 为空）必须先分配，便于 `setData()`
   - 遍历算子时给输出分配
4) 回收：当某个输入 tensor 的 remaining uses 变为 0，且它不是 graph 输出（keepAlive），则将其 offset free 回 allocator
5) 实际分配并绑定：`base = allocator.getPtr()` 后给每个 tensor 绑定 `Blob(base + offset)`

对应代码： [src/core/graph.cc](../src/core/graph.cc)

## 5. 图优化（Graph::optimize）

本作业实现两类非常典型的编译优化：

### 5.1 冗余算子消除：Transpose-Transpose 抵消

若两个连续 transpose 的 permutation 互为逆（inverse），则整体为 identity，可删除两者并把后继使用重定向到原始输入。

这对应编译器的：
- peephole optimization
- algebraic simplification

### 5.2 算子融合：Transpose 融入 Matmul

若 Matmul 的输入来自一个“交换最后两维”的 transpose，则可把 transpose 用 Matmul 的 `transA/transB` 表示，从而移除 transpose。

这对应：
- operator fusion
- layout/transpose pushing

实现工程要点：
- 修改算子输入后，需要**重建图连接关系**（tensor source/targets 与 op pred/succ），避免悬挂引用或旧连接导致的不一致。

对应代码： [src/core/graph.cc](../src/core/graph.cc)

## 6. 工程化要点

- 保持 TODO 注释：作业框架保留了 TODO 区块作为提示，完成实现时不删除，便于对照。
- -Werror 约束：项目把 warning 当成 error，需要注意 signed/unsigned 比较等细节。
- 子模块依赖：googletest 等依赖通过 git submodule 引入；在无 SSH key 环境更推荐 HTTPS URL。

## 7. 如何在 WSL 下自测

在 WSL Ubuntu 中：

```bash
cd /mnt/d/Code/github/InfiniTensor/TinyInfiniTensor
sudo apt-get update
sudo apt-get install -y cmake make g++
make clean
make build TYPE=Release TEST=ON
make test-cpp TYPE=Release
```

预期输出：`100% tests passed, 0 tests failed out of 11`
